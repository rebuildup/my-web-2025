# æœ¬ç•ªç’°å¢ƒå¯¾å¿œã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€æ‹¡å¼µã•ã‚ŒãŸãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’æœ¬ç•ªç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ãŸã‚ã®è¨­å®šã¨æ‰‹é †ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

## ç›®æ¬¡

1. [ç’°å¢ƒå¤‰æ•°ãƒ»è¨­å®šã®èª¿æ•´](#ç’°å¢ƒå¤‰æ•°è¨­å®šã®èª¿æ•´)
2. [ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æº–å‚™](#ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æº–å‚™)
3. [ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ§‹é€ ã®èª¿æ•´](#ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ§‹é€ ã®èª¿æ•´)
4. [ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šã®æœ€é©åŒ–](#ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šã®æœ€é©åŒ–)
5. [ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å®šã®èª¿æ•´](#ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å®šã®èª¿æ•´)
6. [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š](#ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š)
7. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)

---

## ç’°å¢ƒå¤‰æ•°ãƒ»è¨­å®šã®èª¿æ•´

### æœ¬ç•ªç’°å¢ƒç”¨ç’°å¢ƒå¤‰æ•°

**`.env.production`**:

```bash
# åŸºæœ¬è¨­å®š
NODE_ENV=production
PORT=3000
HOST=0.0.0.0

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
DATABASE_URL=postgresql://portfolio_user:${DB_PASSWORD}@${DB_HOST}:5432/portfolio_production
DATABASE_POOL_MIN=2
DATABASE_POOL_MAX=20
DATABASE_TIMEOUT=30000

# Redisè¨­å®šï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ï¼‰
REDIS_URL=redis://${REDIS_HOST}:6379
REDIS_PASSWORD=${REDIS_PASSWORD}
REDIS_DB=0

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®š
UPLOAD_DIR=/var/www/portfolio/uploads
MARKDOWN_DIR=/var/www/portfolio/data/markdown
MAX_FILE_SIZE=52428800  # 50MB
ALLOWED_FILE_TYPES=image/jpeg,image/png,image/gif,image/webp,text/markdown

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
JWT_SECRET=${JWT_SECRET}
SESSION_SECRET=${SESSION_SECRET}
BCRYPT_ROUNDS=12

# å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š
CDN_URL=https://cdn.example.com
SMTP_HOST=${SMTP_HOST}
SMTP_PORT=587
SMTP_USER=${SMTP_USER}
SMTP_PASS=${SMTP_PASS}

# ç›£è¦–ãƒ»ãƒ­ã‚°è¨­å®š
LOG_LEVEL=info
LOG_FILE=/var/log/portfolio/app.log
ERROR_REPORTING_URL=${ERROR_REPORTING_URL}

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
ENABLE_COMPRESSION=true
ENABLE_CACHING=true
CACHE_TTL=3600
STATIC_CACHE_TTL=86400

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼
ENABLE_HELMET=true
ENABLE_CORS=false
CORS_ORIGIN=https://portfolio.example.com

# æ©Ÿèƒ½ãƒ•ãƒ©ã‚°
ENABLE_ANALYTICS=true
ENABLE_ERROR_TRACKING=true
ENABLE_PERFORMANCE_MONITORING=true
```

### Next.js è¨­å®šã®èª¿æ•´

**`next.config.js`**:

```javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  // æœ¬ç•ªç’°å¢ƒæœ€é©åŒ–
  reactStrictMode: true,
  swcMinify: true,

  // ç”»åƒæœ€é©åŒ–
  images: {
    domains: ["cdn.example.com", "localhost"],
    formats: ["image/webp", "image/avif"],
    minimumCacheTTL: 86400, // 24æ™‚é–“
    dangerouslyAllowSVG: false,
  },

  // åœ§ç¸®è¨­å®š
  compress: true,

  // ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼
  async headers() {
    return [
      {
        source: "/(.*)",
        headers: [
          {
            key: "X-Frame-Options",
            value: "DENY",
          },
          {
            key: "X-Content-Type-Options",
            value: "nosniff",
          },
          {
            key: "Referrer-Policy",
            value: "strict-origin-when-cross-origin",
          },
          {
            key: "Permissions-Policy",
            value: "camera=(), microphone=(), geolocation=()",
          },
        ],
      },
      {
        source: "/api/(.*)",
        headers: [
          {
            key: "Cache-Control",
            value: "no-store, max-age=0",
          },
        ],
      },
      {
        source: "/static/(.*)",
        headers: [
          {
            key: "Cache-Control",
            value: "public, max-age=31536000, immutable",
          },
        ],
      },
    ];
  },

  // ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆè¨­å®š
  async redirects() {
    return [
      {
        source: "/admin",
        destination: "/admin/dashboard",
        permanent: true,
      },
    ];
  },

  // å®Ÿé¨“çš„æ©Ÿèƒ½
  experimental: {
    serverComponentsExternalPackages: ["sharp"],
    optimizeCss: true,
  },

  // Webpackè¨­å®š
  webpack: (config, { isServer }) => {
    if (!isServer) {
      // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã§ã®Node.jsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é™¤å¤–
      config.resolve.fallback = {
        ...config.resolve.fallback,
        fs: false,
        path: false,
        os: false,
      };
    }

    // ãƒãƒ³ãƒ‰ãƒ«ã‚µã‚¤ã‚ºæœ€é©åŒ–
    config.optimization.splitChunks = {
      chunks: "all",
      cacheGroups: {
        vendor: {
          test: /[\\/]node_modules[\\/]/,
          name: "vendors",
          chunks: "all",
        },
      },
    };

    return config;
  },

  // å‡ºåŠ›è¨­å®š
  output: "standalone",

  // ç’°å¢ƒå¤‰æ•°ã®å…¬é–‹
  env: {
    CUSTOM_KEY: process.env.CUSTOM_KEY,
  },

  // TypeScriptè¨­å®š
  typescript: {
    ignoreBuildErrors: false,
  },

  // ESLintè¨­å®š
  eslint: {
    ignoreDuringBuilds: false,
  },
};

module.exports = nextConfig;
```

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š

**`database/production-config.js`**:

```javascript
const { Pool } = require("pg");

const productionConfig = {
  // æ¥ç¶šè¨­å®š
  connectionString: process.env.DATABASE_URL,
  ssl: {
    rejectUnauthorized: false,
  },

  // ãƒ—ãƒ¼ãƒ«è¨­å®š
  min: parseInt(process.env.DATABASE_POOL_MIN) || 2,
  max: parseInt(process.env.DATABASE_POOL_MAX) || 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: parseInt(process.env.DATABASE_TIMEOUT) || 30000,

  // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
  statement_timeout: 30000,
  query_timeout: 30000,

  // ãƒ­ã‚°è¨­å®š
  log: (msg) => {
    if (process.env.LOG_LEVEL === "debug") {
      console.log("DB:", msg);
    }
  },
};

// æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®ä½œæˆ
const pool = new Pool(productionConfig);

// æ¥ç¶šã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
pool.on("error", (err) => {
  console.error("Database pool error:", err);
  process.exit(-1);
});

// ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
pool.on("connect", () => {
  console.log("Database connected successfully");
});

module.exports = pool;
```

---

## ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æº–å‚™

### ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆç®¡ç†

**`database/migrations/001_enhanced_content_structure.sql`**:

```sql
-- æ‹¡å¼µã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹é€ ã¸ã®ç§»è¡Œ
-- å®Ÿè¡Œå‰ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹ã“ã¨

BEGIN;

-- 1. æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªãƒ¼å‹ã®è¿½åŠ 
DO $$
BEGIN
    IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'enhanced_category_type') THEN
        CREATE TYPE enhanced_category_type AS ENUM (
            'develop', 'video', 'design', 'video&design', 'other'
        );
    END IF;
END $$;

-- 2. æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
CREATE TABLE IF NOT EXISTS portfolio_items_backup AS
SELECT * FROM portfolio_items;

-- 3. æ–°ã—ã„ã‚«ãƒ©ãƒ ã®è¿½åŠ 
ALTER TABLE portfolio_items
ADD COLUMN IF NOT EXISTS categories enhanced_category_type[] DEFAULT '{}',
ADD COLUMN IF NOT EXISTS is_other_category BOOLEAN DEFAULT FALSE,
ADD COLUMN IF NOT EXISTS manual_date TIMESTAMP,
ADD COLUMN IF NOT EXISTS use_manual_date BOOLEAN DEFAULT FALSE,
ADD COLUMN IF NOT EXISTS original_images TEXT[] DEFAULT '{}',
ADD COLUMN IF NOT EXISTS processed_images TEXT[] DEFAULT '{}',
ADD COLUMN IF NOT EXISTS markdown_path TEXT,
ADD COLUMN IF NOT EXISTS markdown_content TEXT,
ADD COLUMN IF NOT EXISTS effective_date TIMESTAMP,
ADD COLUMN IF NOT EXISTS migration_version INTEGER DEFAULT 1;

-- 4. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ
UPDATE portfolio_items
SET
    categories = ARRAY[category::enhanced_category_type],
    is_other_category = (category = 'other'),
    manual_date = created_at,
    use_manual_date = FALSE,
    processed_images = COALESCE(images, '{}'),
    original_images = '{}',
    effective_date = created_at,
    migration_version = 2
WHERE categories = '{}' OR categories IS NULL;

-- 5. ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆï¼ˆå¤–éƒ¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å®Ÿè¡Œï¼‰
-- ã“ã®éƒ¨åˆ†ã¯åˆ¥é€”Node.jsã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å®Ÿè¡Œ

-- 6. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
CREATE INDEX IF NOT EXISTS idx_portfolio_items_categories
ON portfolio_items USING GIN(categories);

CREATE INDEX IF NOT EXISTS idx_portfolio_items_effective_date
ON portfolio_items(effective_date DESC);

CREATE INDEX IF NOT EXISTS idx_portfolio_items_is_other
ON portfolio_items(is_other_category);

CREATE INDEX IF NOT EXISTS idx_portfolio_items_status_categories
ON portfolio_items(status, categories);

-- 7. ã‚¿ã‚°ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ
CREATE TABLE IF NOT EXISTS tag_info (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,
    count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_used TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 8. æ—¢å­˜ã‚¿ã‚°ã®ç§»è¡Œ
INSERT INTO tag_info (name, count, created_at, last_used)
SELECT
    DISTINCT unnest(tags) as tag_name,
    COUNT(*) as usage_count,
    MIN(created_at) as first_used,
    MAX(updated_at) as last_used
FROM portfolio_items
WHERE tags IS NOT NULL AND array_length(tags, 1) > 0
GROUP BY unnest(tags)
ON CONFLICT (name) DO UPDATE SET
    count = EXCLUDED.count,
    last_used = EXCLUDED.last_used;

-- 9. åˆ¶ç´„ã®è¿½åŠ 
ALTER TABLE portfolio_items
ADD CONSTRAINT chk_categories_not_empty
CHECK (array_length(categories, 1) > 0);

ALTER TABLE portfolio_items
ADD CONSTRAINT chk_manual_date_logic
CHECK (
    (use_manual_date = TRUE AND manual_date IS NOT NULL) OR
    (use_manual_date = FALSE)
);

-- 10. ãƒˆãƒªã‚¬ãƒ¼ã®ä½œæˆï¼ˆeffective_dateè‡ªå‹•æ›´æ–°ï¼‰
CREATE OR REPLACE FUNCTION update_effective_date()
RETURNS TRIGGER AS $$
BEGIN
    NEW.effective_date = CASE
        WHEN NEW.use_manual_date = TRUE AND NEW.manual_date IS NOT NULL
        THEN NEW.manual_date
        ELSE NEW.updated_at
    END;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

DROP TRIGGER IF EXISTS trigger_update_effective_date ON portfolio_items;
CREATE TRIGGER trigger_update_effective_date
    BEFORE INSERT OR UPDATE ON portfolio_items
    FOR EACH ROW
    EXECUTE FUNCTION update_effective_date();

-- 11. çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
ANALYZE portfolio_items;
ANALYZE tag_info;

COMMIT;
```

### ç§»è¡Œå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**`database/migrate-production.js`**:

```javascript
#!/usr/bin/env node

const fs = require("fs").promises;
const path = require("path");
const { Pool } = require("pg");

class ProductionMigrator {
  constructor() {
    this.pool = new Pool({
      connectionString: process.env.DATABASE_URL,
      ssl: { rejectUnauthorized: false },
    });

    this.migrationDir = path.join(__dirname, "migrations");
    this.backupDir = path.join(__dirname, "backups");
  }

  async migrate() {
    console.log("ğŸš€ Starting production database migration...");

    try {
      // 1. å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
      await this.checkPrerequisites();

      // 2. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
      await this.createBackup();

      // 3. ç§»è¡Œå®Ÿè¡Œ
      await this.executeMigrations();

      // 4. ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œ
      await this.migrateMarkdownFiles();

      // 5. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
      await this.verifyMigration();

      console.log("âœ… Migration completed successfully");
    } catch (error) {
      console.error("âŒ Migration failed:", error);
      await this.rollback();
      process.exit(1);
    } finally {
      await this.pool.end();
    }
  }

  async checkPrerequisites() {
    console.log("ğŸ” Checking prerequisites...");

    // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
    const client = await this.pool.connect();
    try {
      await client.query("SELECT 1");
      console.log("âœ… Database connection OK");
    } finally {
      client.release();
    }

    // å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    await fs.mkdir(this.backupDir, { recursive: true });
    await fs.mkdir(process.env.MARKDOWN_DIR, { recursive: true });

    // æ¨©é™ç¢ºèª
    await fs.access(process.env.UPLOAD_DIR, fs.constants.W_OK);
    await fs.access(process.env.MARKDOWN_DIR, fs.constants.W_OK);

    console.log("âœ… Prerequisites check passed");
  }

  async createBackup() {
    console.log("ğŸ’¾ Creating database backup...");

    const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
    const backupFile = path.join(this.backupDir, `backup-${timestamp}.sql`);

    const { spawn } = require("child_process");

    return new Promise((resolve, reject) => {
      const pg_dump = spawn("pg_dump", [process.env.DATABASE_URL], {
        stdio: ["ignore", "pipe", "pipe"],
      });

      const writeStream = require("fs").createWriteStream(backupFile);
      pg_dump.stdout.pipe(writeStream);

      pg_dump.on("close", (code) => {
        if (code === 0) {
          console.log(`âœ… Backup created: ${backupFile}`);
          resolve(backupFile);
        } else {
          reject(new Error(`pg_dump failed with code ${code}`));
        }
      });

      pg_dump.on("error", reject);
    });
  }

  async executeMigrations() {
    console.log("ğŸ”„ Executing migrations...");

    const migrationFiles = await fs.readdir(this.migrationDir);
    const sqlFiles = migrationFiles
      .filter((file) => file.endsWith(".sql"))
      .sort();

    for (const file of sqlFiles) {
      console.log(`  ğŸ“„ Executing ${file}...`);

      const filePath = path.join(this.migrationDir, file);
      const sql = await fs.readFile(filePath, "utf8");

      const client = await this.pool.connect();
      try {
        await client.query(sql);
        console.log(`  âœ… ${file} completed`);
      } catch (error) {
        console.error(`  âŒ ${file} failed:`, error);
        throw error;
      } finally {
        client.release();
      }
    }
  }

  async migrateMarkdownFiles() {
    console.log("ğŸ“ Migrating markdown content to files...");

    const client = await this.pool.connect();
    try {
      const result = await client.query(`
        SELECT id, content 
        FROM portfolio_items 
        WHERE content IS NOT NULL 
        AND content != '' 
        AND markdown_path IS NULL
      `);

      for (const row of result.rows) {
        const fileName = `${row.id}.md`;
        const filePath = path.join(process.env.MARKDOWN_DIR, fileName);

        // ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
        await fs.writeFile(filePath, row.content, "utf8");

        // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ›´æ–°
        await client.query(
          `
          UPDATE portfolio_items 
          SET markdown_path = $1, markdown_content = $2
          WHERE id = $3
        `,
          [filePath, row.content, row.id],
        );

        console.log(`  âœ… Created ${fileName}`);
      }

      console.log(`âœ… Migrated ${result.rows.length} markdown files`);
    } finally {
      client.release();
    }
  }

  async verifyMigration() {
    console.log("ğŸ” Verifying migration...");

    const client = await this.pool.connect();
    try {
      // ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
      const checks = [
        {
          name: "Categories migration",
          query: `SELECT COUNT(*) as count FROM portfolio_items WHERE categories = '{}'`,
          expected: 0,
        },
        {
          name: "Effective date calculation",
          query: `SELECT COUNT(*) as count FROM portfolio_items WHERE effective_date IS NULL`,
          expected: 0,
        },
        {
          name: "Tag info population",
          query: `SELECT COUNT(*) as count FROM tag_info`,
          expected: "> 0",
        },
      ];

      for (const check of checks) {
        const result = await client.query(check.query);
        const count = parseInt(result.rows[0].count);

        if (check.expected === "> 0" ? count > 0 : count === check.expected) {
          console.log(`  âœ… ${check.name}: OK`);
        } else {
          throw new Error(
            `${check.name} failed: expected ${check.expected}, got ${count}`,
          );
        }
      }

      console.log("âœ… Migration verification passed");
    } finally {
      client.release();
    }
  }

  async rollback() {
    console.log("ğŸ”„ Rolling back migration...");

    // æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    const backupFiles = await fs.readdir(this.backupDir);
    const latestBackup = backupFiles
      .filter((file) => file.startsWith("backup-") && file.endsWith(".sql"))
      .sort()
      .pop();

    if (!latestBackup) {
      console.error("âŒ No backup file found for rollback");
      return;
    }

    const backupPath = path.join(this.backupDir, latestBackup);
    console.log(`ğŸ“„ Restoring from ${latestBackup}...`);

    // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©å…ƒï¼ˆå®Ÿè£…ã¯ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´ï¼‰
    console.log("âš ï¸ Manual database restore required from:", backupPath);
  }
}

// ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
if (require.main === module) {
  const migrator = new ProductionMigrator();
  migrator.migrate();
}

module.exports = ProductionMigrator;
```

---

## ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ§‹é€ ã®èª¿æ•´

### æœ¬ç•ªç’°å¢ƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
/var/www/portfolio/
â”œâ”€â”€ app/                          # Next.js ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ markdown/            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”œâ”€â”€ uploads/                 # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”‚   â”œâ”€â”€ original/        # å¤‰æ›ãªã—ç”»åƒ
â”‚   â”‚   â”‚   â”œâ”€â”€ processed/       # å‡¦ç†æ¸ˆã¿ç”»åƒ
â”‚   â”‚   â”‚   â””â”€â”€ thumbnails/      # ã‚µãƒ ãƒã‚¤ãƒ«
â”‚   â”‚   â””â”€â”€ temp/               # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â””â”€â”€ static/                 # é™çš„ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ logs/                       # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”œâ”€â”€ app.log
â”‚   â”œâ”€â”€ error.log
â”‚   â””â”€â”€ access.log
â”œâ”€â”€ backups/                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
â”‚   â”œâ”€â”€ database/
â”‚   â””â”€â”€ files/
â””â”€â”€ config/                     # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    â”œâ”€â”€ nginx/
    â””â”€â”€ ssl/
```

### ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**`deployment/setup-filesystem.sh`**:

```bash
#!/bin/bash

# æœ¬ç•ªç’°å¢ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ

set -e

# è¨­å®š
APP_DIR="/var/www/portfolio"
APP_USER="portfolio"
WEB_USER="www-data"

echo "ğŸš€ Setting up production filesystem..."

# 1. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
echo "ğŸ“ Creating directories..."
sudo mkdir -p $APP_DIR/{public/{data/markdown,uploads/{images/{original,processed,thumbnails},temp},static},logs,backups/{database,files},config/{nginx,ssl}}

# 2. æ¨©é™è¨­å®š
echo "ğŸ” Setting permissions..."

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
sudo chown -R $APP_USER:$APP_USER $APP_DIR
sudo chmod -R 755 $APP_DIR

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ›¸ãè¾¼ã¿æ¨©é™ï¼‰
sudo chown -R $WEB_USER:$WEB_USER $APP_DIR/public/uploads
sudo chmod -R 775 $APP_DIR/public/uploads

# ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ›¸ãè¾¼ã¿æ¨©é™ï¼‰
sudo chown -R $WEB_USER:$WEB_USER $APP_DIR/public/data
sudo chmod -R 775 $APP_DIR/public/data

# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ›¸ãè¾¼ã¿æ¨©é™ï¼‰
sudo chown -R $WEB_USER:$WEB_USER $APP_DIR/logs
sudo chmod -R 775 $APP_DIR/logs

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
sudo chown -R $APP_USER:$APP_USER $APP_DIR/backups
sudo chmod -R 750 $APP_DIR/backups

# è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰
sudo chown -R root:$APP_USER $APP_DIR/config
sudo chmod -R 640 $APP_DIR/config

# 3. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
echo "ğŸ”’ Applying security settings..."

# å®Ÿè¡Œæ¨©é™ã®é™¤å»ï¼ˆå¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ä»¥å¤–ï¼‰
find $APP_DIR/public/uploads -type f -exec chmod 644 {} \;
find $APP_DIR/public/data -type f -exec chmod 644 {} \;

# éš ã—ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿è­·
find $APP_DIR -name ".*" -type f -exec chmod 600 {} \;

# 4. ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
echo "ğŸ“‹ Setting up log rotation..."
sudo tee /etc/logrotate.d/portfolio > /dev/null <<EOF
$APP_DIR/logs/*.log {
    daily
    missingok
    rotate 30
    compress
    delaycompress
    notifempty
    create 644 $WEB_USER $WEB_USER
    postrotate
        systemctl reload nginx > /dev/null 2>&1 || true
        systemctl reload portfolio-app > /dev/null 2>&1 || true
    endscript
}
EOF

# 5. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®cronè¨­å®š
echo "ğŸ§¹ Setting up cleanup cron..."
sudo tee /etc/cron.d/portfolio-cleanup > /dev/null <<EOF
# Portfolio temporary files cleanup
0 2 * * * $WEB_USER find $APP_DIR/public/uploads/temp -type f -mtime +1 -delete
30 2 * * * $WEB_USER find $APP_DIR/logs -name "*.log.*" -mtime +30 -delete
EOF

# 6. ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ç›£è¦–
echo "ğŸ“Š Setting up disk monitoring..."
sudo tee /etc/cron.d/portfolio-disk-monitor > /dev/null <<EOF
# Portfolio disk usage monitoring
0 */6 * * * root df -h $APP_DIR | awk 'NR==2 {if(substr(\$5,1,length(\$5)-1) > 80) print "WARNING: Portfolio disk usage is " \$5}' | mail -s "Disk Usage Alert" admin@example.com
EOF

# 7. ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
echo "ğŸ” Setting up integrity checks..."
sudo tee /usr/local/bin/portfolio-integrity-check > /dev/null <<'EOF'
#!/bin/bash

# Portfolio filesystem integrity check
APP_DIR="/var/www/portfolio"
LOG_FILE="$APP_DIR/logs/integrity-check.log"

echo "$(date): Starting integrity check" >> $LOG_FILE

# Check critical directories
CRITICAL_DIRS=("$APP_DIR/public/uploads" "$APP_DIR/public/data" "$APP_DIR/logs")

for dir in "${CRITICAL_DIRS[@]}"; do
    if [ ! -d "$dir" ]; then
        echo "$(date): ERROR - Missing directory: $dir" >> $LOG_FILE
        echo "CRITICAL: Missing directory $dir" | mail -s "Portfolio Integrity Alert" admin@example.com
    fi
done

# Check permissions
if [ ! -w "$APP_DIR/public/uploads" ]; then
    echo "$(date): ERROR - Upload directory not writable" >> $LOG_FILE
    echo "CRITICAL: Upload directory permission issue" | mail -s "Portfolio Permission Alert" admin@example.com
fi

# Check disk space
DISK_USAGE=$(df $APP_DIR | awk 'NR==2 {print substr($5,1,length($5)-1)}')
if [ $DISK_USAGE -gt 90 ]; then
    echo "$(date): WARNING - Disk usage is ${DISK_USAGE}%" >> $LOG_FILE
    echo "WARNING: Disk usage is ${DISK_USAGE}%" | mail -s "Portfolio Disk Alert" admin@example.com
fi

echo "$(date): Integrity check completed" >> $LOG_FILE
EOF

sudo chmod +x /usr/local/bin/portfolio-integrity-check

# æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã®cronè¨­å®š
sudo tee -a /etc/cron.d/portfolio-cleanup > /dev/null <<EOF
# Portfolio integrity check
0 4 * * * root /usr/local/bin/portfolio-integrity-check
EOF

echo "âœ… Filesystem setup completed"
echo "ğŸ“‹ Summary:"
echo "  - Application directory: $APP_DIR"
echo "  - Upload directory: $APP_DIR/public/uploads"
echo "  - Markdown directory: $APP_DIR/public/data/markdown"
echo "  - Log directory: $APP_DIR/logs"
echo "  - Backup directory: $APP_DIR/backups"
echo "  - Log rotation: Configured"
echo "  - Cleanup cron: Configured"
echo "  - Integrity check: Configured"
```

---

## ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šã®æœ€é©åŒ–

### Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š

**`config/redis-production.conf`**:

```conf
# Redis æœ¬ç•ªç’°å¢ƒè¨­å®š

# åŸºæœ¬è¨­å®š
port 6379
bind 127.0.0.1
protected-mode yes
requirepass ${REDIS_PASSWORD}

# ãƒ¡ãƒ¢ãƒªè¨­å®š
maxmemory 1gb
maxmemory-policy allkeys-lru

# æ°¸ç¶šåŒ–è¨­å®š
save 900 1
save 300 10
save 60 10000

# ãƒ­ã‚°è¨­å®š
loglevel notice
logfile /var/log/redis/redis-server.log

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
rename-command FLUSHDB ""
rename-command FLUSHALL ""
rename-command DEBUG ""
rename-command CONFIG "CONFIG_b835c3f8a5d2e7f1"

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
tcp-keepalive 300
timeout 0
tcp-backlog 511

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
maxclients 10000

# ã‚¹ãƒ­ãƒ¼ãƒ­ã‚°è¨­å®š
slowlog-log-slower-than 10000
slowlog-max-len 128
```

### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š

**`lib/cache/production-cache.js`**:

```javascript
const Redis = require("redis");

class ProductionCache {
  constructor() {
    this.redis = Redis.createClient({
      url: process.env.REDIS_URL,
      password: process.env.REDIS_PASSWORD,
      retry_strategy: (options) => {
        if (options.error && options.error.code === "ECONNREFUSED") {
          return new Error("Redis server connection refused");
        }
        if (options.total_retry_time > 1000 * 60 * 60) {
          return new Error("Redis retry time exhausted");
        }
        if (options.attempt > 10) {
          return undefined;
        }
        return Math.min(options.attempt * 100, 3000);
      },
    });

    this.redis.on("error", (err) => {
      console.error("Redis error:", err);
    });

    this.redis.on("connect", () => {
      console.log("Redis connected");
    });
  }

  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥è¨­å®š
  getCacheConfig() {
    return {
      // ãƒšãƒ¼ã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥
      pages: {
        gallery: { ttl: 300 }, // 5åˆ†
        portfolioItem: { ttl: 600 }, // 10åˆ†
        admin: { ttl: 0 }, // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãªã„
      },

      // APIã‚­ãƒ£ãƒƒã‚·ãƒ¥
      api: {
        content: { ttl: 300 }, // 5åˆ†
        tags: { ttl: 1800 }, // 30åˆ†
        search: { ttl: 600 }, // 10åˆ†
      },

      // é™çš„ãƒªã‚½ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥
      static: {
        images: { ttl: 86400 }, // 24æ™‚é–“
        css: { ttl: 86400 }, // 24æ™‚é–“
        js: { ttl: 86400 }, // 24æ™‚é–“
      },
    };
  }

  async get(key) {
    try {
      const value = await this.redis.get(key);
      return value ? JSON.parse(value) : null;
    } catch (error) {
      console.error("Cache get error:", error);
      return null;
    }
  }

  async set(key, value, ttl = 3600) {
    try {
      const serialized = JSON.stringify(value);
      if (ttl > 0) {
        await this.redis.setex(key, ttl, serialized);
      } else {
        await this.redis.set(key, serialized);
      }
      return true;
    } catch (error) {
      console.error("Cache set error:", error);
      return false;
    }
  }

  async del(key) {
    try {
      await this.redis.del(key);
      return true;
    } catch (error) {
      console.error("Cache delete error:", error);
      return false;
    }
  }

  async flush() {
    try {
      await this.redis.flushdb();
      return true;
    } catch (error) {
      console.error("Cache flush error:", error);
      return false;
    }
  }

  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–æˆ¦ç•¥
  async invalidatePattern(pattern) {
    try {
      const keys = await this.redis.keys(pattern);
      if (keys.length > 0) {
        await this.redis.del(...keys);
      }
      return true;
    } catch (error) {
      console.error("Cache invalidation error:", error);
      return false;
    }
  }

  // ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
  async healthCheck() {
    try {
      await this.redis.ping();
      return { status: "healthy", timestamp: new Date() };
    } catch (error) {
      return {
        status: "unhealthy",
        error: error.message,
        timestamp: new Date(),
      };
    }
  }
}

module.exports = new ProductionCache();
```

### Nginx ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š

**`config/nginx/portfolio.conf`**:

```nginx
# Portfolio Nginx è¨­å®š

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹è¨­å®š
proxy_cache_path /var/cache/nginx/portfolio levels=1:2 keys_zone=portfolio:10m max_size=1g inactive=60m use_temp_path=off;

# ã‚¢ãƒƒãƒ—ã‚¹ãƒˆãƒªãƒ¼ãƒ è¨­å®š
upstream portfolio_app {
    server 127.0.0.1:3000;
    keepalive 32;
}

server {
    listen 80;
    server_name portfolio.example.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name portfolio.example.com;

    # SSLè¨­å®š
    ssl_certificate /etc/ssl/certs/portfolio.crt;
    ssl_certificate_key /etc/ssl/private/portfolio.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;
    ssl_prefer_server_ciphers off;
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 10m;

    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;

    # ãƒ­ã‚°è¨­å®š
    access_log /var/log/nginx/portfolio_access.log;
    error_log /var/log/nginx/portfolio_error.log;

    # é™çš„ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡
    location /static/ {
        alias /var/www/portfolio/public/static/;
        expires 1y;
        add_header Cache-Control "public, immutable";
        gzip_static on;
    }

    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡
    location /uploads/ {
        alias /var/www/portfolio/public/uploads/;
        expires 30d;
        add_header Cache-Control "public";

        # ç”»åƒæœ€é©åŒ–
        location ~* \.(jpg|jpeg|png|gif|webp)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }

    # API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
    location /api/ {
        proxy_pass http://portfolio_app;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
    }

    # ãƒšãƒ¼ã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    location / {
        proxy_pass http://portfolio_app;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        proxy_cache portfolio;
        proxy_cache_valid 200 5m;
        proxy_cache_valid 404 1m;
        proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504;
        proxy_cache_background_update on;
        proxy_cache_lock on;

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        proxy_cache_key "$scheme$request_method$host$request_uri";

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚¤ãƒ‘ã‚¹æ¡ä»¶
        proxy_cache_bypass $cookie_nocache $arg_nocache $arg_comment;
        proxy_no_cache $cookie_nocache $arg_nocache $arg_comment;

        # ç®¡ç†ç”»é¢ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãªã„
        location /admin {
            proxy_pass http://portfolio_app;
            proxy_cache off;
        }
    }

    # åœ§ç¸®è¨­å®š
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml
        image/svg+xml;

    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=upload:10m rate=1r/s;

    location /api/admin/upload {
        limit_req zone=upload burst=5 nodelay;
        proxy_pass http://portfolio_app;
    }

    location /api/ {
        limit_req zone=api burst=20 nodelay;
        proxy_pass http://portfolio_app;
    }
}
```

ã“ã®ã‚¬ã‚¤ãƒ‰ã«ã‚ˆã‚Šã€æœ¬ç•ªç’°å¢ƒã§ã®å®‰å®šã—ãŸé‹ç”¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å®šã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¾ã™ã€‚

---

## ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å®šã®èª¿æ•´

### ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–è¨­å®š

**`config/monitoring/prometheus.yml`**:

```yaml
# Prometheus è¨­å®š
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "portfolio_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

scrape_configs:
  # Portfolio ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
  - job_name: "portfolio-app"
    static_configs:
      - targets: ["localhost:3000"]
    metrics_path: "/api/metrics"
    scrape_interval: 30s

  # Node Exporterï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
  - job_name: "node-exporter"
    static_configs:
      - targets: ["localhost:9100"]

  # PostgreSQL
  - job_name: "postgres-exporter"
    static_configs:
      - targets: ["localhost:9187"]

  # Redis
  - job_name: "redis-exporter"
    static_configs:
      - targets: ["localhost:9121"]

  # Nginx
  - job_name: "nginx-exporter"
    static_configs:
      - targets: ["localhost:9113"]
```

**`config/monitoring/portfolio_rules.yml`**:

```yaml
# Portfolio ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«
groups:
  - name: portfolio.rules
    rules:
      # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å¯ç”¨æ€§
      - alert: PortfolioAppDown
        expr: up{job="portfolio-app"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Portfolio application is down"
          description: "Portfolio application has been down for more than 1 minute"

      # é«˜ã„ã‚¨ãƒ©ãƒ¼ç‡
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"

      # å¿œç­”æ™‚é–“ã®æ‚ªåŒ–
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time"
          description: "95th percentile response time is {{ $value }}s"

      # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå•é¡Œ
      - alert: DatabaseConnectionIssue
        expr: up{job="postgres-exporter"} == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "Database connection issue"
          description: "Cannot connect to PostgreSQL database"

      # Redisæ¥ç¶šå•é¡Œ
      - alert: RedisConnectionIssue
        expr: up{job="redis-exporter"} == 0
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "Redis connection issue"
          description: "Cannot connect to Redis cache"

      # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡
      - alert: HighDiskUsage
        expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage"
          description: "Disk usage is {{ $value | humanizePercentage }}"

      # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }}"
```

### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹

**`lib/monitoring/metrics.js`**:

```javascript
const client = require("prom-client");

// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åé›†
client.collectDefaultMetrics({ timeout: 5000 });

// ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
const httpRequestsTotal = new client.Counter({
  name: "http_requests_total",
  help: "Total number of HTTP requests",
  labelNames: ["method", "route", "status"],
});

const httpRequestDuration = new client.Histogram({
  name: "http_request_duration_seconds",
  help: "Duration of HTTP requests in seconds",
  labelNames: ["method", "route", "status"],
  buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10],
});

const portfolioItemsTotal = new client.Gauge({
  name: "portfolio_items_total",
  help: "Total number of portfolio items",
  labelNames: ["status", "category"],
});

const fileUploadsTotal = new client.Counter({
  name: "file_uploads_total",
  help: "Total number of file uploads",
  labelNames: ["type", "status"],
});

const cacheHitRate = new client.Gauge({
  name: "cache_hit_rate",
  help: "Cache hit rate",
  labelNames: ["cache_type"],
});

const databaseConnectionPool = new client.Gauge({
  name: "database_connection_pool_size",
  help: "Database connection pool size",
  labelNames: ["state"],
});

// ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°é–¢æ•°
class MetricsCollector {
  static recordHttpRequest(method, route, status, duration) {
    httpRequestsTotal.inc({ method, route, status });
    httpRequestDuration.observe({ method, route, status }, duration);
  }

  static updatePortfolioItemsCount(status, category, count) {
    portfolioItemsTotal.set({ status, category }, count);
  }

  static recordFileUpload(type, status) {
    fileUploadsTotal.inc({ type, status });
  }

  static updateCacheHitRate(cacheType, rate) {
    cacheHitRate.set({ cache_type: cacheType }, rate);
  }

  static updateDatabasePool(active, idle, waiting) {
    databaseConnectionPool.set({ state: "active" }, active);
    databaseConnectionPool.set({ state: "idle" }, idle);
    databaseConnectionPool.set({ state: "waiting" }, waiting);
  }

  static async collectCustomMetrics() {
    try {
      // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†
      const db = require("../database/connection");

      // ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚¢ã‚¤ãƒ†ãƒ æ•°
      const itemCounts = await db.query(`
        SELECT status, unnest(categories) as category, COUNT(*) as count
        FROM portfolio_items 
        GROUP BY status, category
      `);

      itemCounts.rows.forEach((row) => {
        this.updatePortfolioItemsCount(
          row.status,
          row.category,
          parseInt(row.count),
        );
      });

      // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«çŠ¶æ…‹
      const poolStats = db.pool.totalCount;
      const activeConnections = db.pool.activeCount;
      const idleConnections = db.pool.idleCount;
      const waitingCount = db.pool.waitingCount;

      this.updateDatabasePool(activeConnections, idleConnections, waitingCount);
    } catch (error) {
      console.error("Failed to collect custom metrics:", error);
    }
  }

  static getRegistry() {
    return client.register;
  }
}

// å®šæœŸçš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
setInterval(() => {
  MetricsCollector.collectCustomMetrics();
}, 30000); // 30ç§’é–“éš”

module.exports = MetricsCollector;
```

### ãƒ­ã‚°é›†ç´„è¨­å®š

**`config/logging/logstash-production.conf`**:

```ruby
# Logstash æœ¬ç•ªç’°å¢ƒè¨­å®š

input {
  # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
  file {
    path => "/var/www/portfolio/logs/app.log"
    start_position => "beginning"
    codec => json
    type => "application"
    tags => ["portfolio", "application"]
  }

  # Nginxã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°
  file {
    path => "/var/log/nginx/portfolio_access.log"
    start_position => "beginning"
    type => "nginx_access"
    tags => ["portfolio", "nginx", "access"]
  }

  # Nginxã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
  file {
    path => "/var/log/nginx/portfolio_error.log"
    start_position => "beginning"
    type => "nginx_error"
    tags => ["portfolio", "nginx", "error"]
  }

  # ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°
  syslog {
    port => 5514
    type => "syslog"
    tags => ["portfolio", "system"]
  }
}

filter {
  # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ã®å‡¦ç†
  if [type] == "application" {
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®è§£æ
    date {
      match => [ "timestamp", "ISO8601" ]
    }

    # ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ™ãƒ«ã®åˆ†é¡
    if [level] == "ERROR" {
      mutate {
        add_tag => ["error"]
      }
    }

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é–¢é€£ãƒ­ã‚°ã®è­˜åˆ¥
    if [message] =~ /response_time|duration/ {
      mutate {
        add_tag => ["performance"]
      }
    }

    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£ãƒ­ã‚°ã®è­˜åˆ¥
    if [message] =~ /authentication|authorization|security/ {
      mutate {
        add_tag => ["security"]
      }
    }
  }

  # Nginxã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã®å‡¦ç†
  if [type] == "nginx_access" {
    grok {
      match => {
        "message" => "%{NGINXACCESS}"
      }
    }

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®æ•°å€¤åŒ–
    mutate {
      convert => { "response_time" => "float" }
      convert => { "status" => "integer" }
    }

    # ã‚¨ãƒ©ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®è­˜åˆ¥
    if [status] >= 400 {
      mutate {
        add_tag => ["error"]
      }
    }

    # ã‚¹ãƒ­ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®è­˜åˆ¥
    if [response_time] > 2.0 {
      mutate {
        add_tag => ["slow_request"]
      }
    }
  }

  # åœ°ç†çš„ä½ç½®æƒ…å ±ã®è¿½åŠ 
  if [clientip] {
    geoip {
      source => "clientip"
      target => "geoip"
    }
  }

  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è§£æ
  if [agent] {
    useragent {
      source => "agent"
      target => "useragent"
    }
  }
}

output {
  # Elasticsearch ã¸ã®å‡ºåŠ›
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "portfolio-logs-%{+YYYY.MM.dd}"
    template_name => "portfolio"
    template_pattern => "portfolio-*"
    template => "/etc/logstash/templates/portfolio-template.json"
  }

  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®å³åº§é€šçŸ¥
  if "error" in [tags] {
    email {
      to => "admin@example.com"
      subject => "Portfolio Error Alert"
      body => "Error detected in portfolio system: %{message}"
      from => "alerts@example.com"
    }
  }

  # é‡è¦ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆã®é€šçŸ¥
  if "security" in [tags] and [level] == "ERROR" {
    http {
      url => "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
      http_method => "post"
      content_type => "application/json"
      format => "json"
      mapping => {
        "text" => "Security alert: %{message}"
        "channel" => "#security-alerts"
      }
    }
  }

  # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼ˆé–‹ç™ºæ™‚ã®ã¿ï¼‰
  if [type] == "application" and [level] == "DEBUG" {
    stdout {
      codec => rubydebug
    }
  }
}
```

### ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š

**`config/monitoring/grafana-dashboard.json`**:

```json
{
  "dashboard": {
    "id": null,
    "title": "Portfolio System Dashboard",
    "tags": ["portfolio", "production"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Application Status",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"portfolio-app\"}",
            "legendFormat": "App Status"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                { "color": "red", "value": 0 },
                { "color": "green", "value": 1 }
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{route}}"
          }
        ]
      },
      {
        "id": 3,
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "id": 4,
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m])",
            "legendFormat": "5xx errors"
          },
          {
            "expr": "rate(http_requests_total{status=~\"4..\"}[5m])",
            "legendFormat": "4xx errors"
          }
        ]
      },
      {
        "id": 5,
        "title": "System Resources",
        "type": "graph",
        "targets": [
          {
            "expr": "100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage %"
          },
          {
            "expr": "(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100",
            "legendFormat": "Memory Usage %"
          }
        ]
      },
      {
        "id": 6,
        "title": "Database Connections",
        "type": "graph",
        "targets": [
          {
            "expr": "database_connection_pool_size{state=\"active\"}",
            "legendFormat": "Active"
          },
          {
            "expr": "database_connection_pool_size{state=\"idle\"}",
            "legendFormat": "Idle"
          }
        ]
      },
      {
        "id": 7,
        "title": "Cache Hit Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "cache_hit_rate",
            "legendFormat": "{{cache_type}}"
          }
        ]
      },
      {
        "id": 8,
        "title": "File Uploads",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(file_uploads_total[5m])",
            "legendFormat": "{{type}} - {{status}}"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

---

## ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

### SSL/TLS è¨­å®š

**`deployment/setup-ssl.sh`**:

```bash
#!/bin/bash

# SSLè¨¼æ˜æ›¸è¨­å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ

set -e

DOMAIN="portfolio.example.com"
SSL_DIR="/etc/ssl/portfolio"
CERT_EMAIL="admin@example.com"

echo "ğŸ”’ Setting up SSL certificates..."

# 1. Let's Encryptè¨¼æ˜æ›¸ã®å–å¾—
if command -v certbot >/dev/null 2>&1; then
    echo "ğŸ“œ Obtaining Let's Encrypt certificate..."

    # Certbot ã§SSLè¨¼æ˜æ›¸å–å¾—
    sudo certbot certonly \
        --nginx \
        --email $CERT_EMAIL \
        --agree-tos \
        --no-eff-email \
        --domains $DOMAIN

    # è¨¼æ˜æ›¸ã®è‡ªå‹•æ›´æ–°è¨­å®š
    echo "0 12 * * * /usr/bin/certbot renew --quiet" | sudo crontab -

else
    echo "âš ï¸ Certbot not found. Setting up self-signed certificate for testing..."

    # è‡ªå·±ç½²åè¨¼æ˜æ›¸ã®ä½œæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    sudo mkdir -p $SSL_DIR

    sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
        -keyout $SSL_DIR/portfolio.key \
        -out $SSL_DIR/portfolio.crt \
        -subj "/C=JP/ST=Tokyo/L=Tokyo/O=Portfolio/CN=$DOMAIN"

    sudo chmod 600 $SSL_DIR/portfolio.key
    sudo chmod 644 $SSL_DIR/portfolio.crt
fi

# 2. SSLè¨­å®šã®å¼·åŒ–
echo "ğŸ”§ Configuring SSL security..."

# DH ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç”Ÿæˆ
sudo openssl dhparam -out /etc/ssl/certs/dhparam.pem 2048

# SSLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
sudo tee /etc/nginx/snippets/ssl-portfolio.conf > /dev/null <<EOF
# SSL Configuration for Portfolio
ssl_certificate /etc/letsencrypt/live/$DOMAIN/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/$DOMAIN/privkey.pem;

# SSL Security
ssl_protocols TLSv1.2 TLSv1.3;
ssl_prefer_server_ciphers off;
ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384;

# SSL Session
ssl_session_timeout 1d;
ssl_session_cache shared:SSL:50m;
ssl_stapling on;
ssl_stapling_verify on;

# DH Parameters
ssl_dhparam /etc/ssl/certs/dhparam.pem;

# HSTS
add_header Strict-Transport-Security "max-age=63072000" always;
EOF

echo "âœ… SSL setup completed"
```

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š

**`middleware/security-headers.js`**:

```javascript
const helmet = require("helmet");

const securityHeaders = helmet({
  // Content Security Policy
  contentSecurityPolicy: {
    directives: {
      defaultSrc: ["'self'"],
      scriptSrc: [
        "'self'",
        "'unsafe-inline'", // Next.js requires this
        "'unsafe-eval'", // Development only
        "https://cdn.jsdelivr.net",
        "https://unpkg.com",
      ],
      styleSrc: ["'self'", "'unsafe-inline'", "https://fonts.googleapis.com"],
      fontSrc: ["'self'", "https://fonts.gstatic.com"],
      imgSrc: ["'self'", "data:", "https:", "blob:"],
      connectSrc: ["'self'", "https://api.example.com"],
      mediaSrc: ["'self'"],
      objectSrc: ["'none'"],
      frameSrc: ["'none'"],
      baseUri: ["'self'"],
      formAction: ["'self'"],
      frameAncestors: ["'none'"],
      upgradeInsecureRequests: [],
    },
  },

  // Cross Origin Embedder Policy
  crossOriginEmbedderPolicy: false,

  // Cross Origin Opener Policy
  crossOriginOpenerPolicy: { policy: "same-origin" },

  // Cross Origin Resource Policy
  crossOriginResourcePolicy: { policy: "cross-origin" },

  // DNS Prefetch Control
  dnsPrefetchControl: { allow: false },

  // Expect Certificate Transparency
  expectCt: {
    maxAge: 86400,
    enforce: true,
  },

  // Feature Policy / Permissions Policy
  permittedCrossDomainPolicies: false,

  // Hide Powered By
  hidePoweredBy: true,

  // HSTS
  hsts: {
    maxAge: 31536000,
    includeSubDomains: true,
    preload: true,
  },

  // IE No Open
  ieNoOpen: true,

  // No Sniff
  noSniff: true,

  // Origin Agent Cluster
  originAgentCluster: true,

  // Referrer Policy
  referrerPolicy: { policy: "strict-origin-when-cross-origin" },

  // X-Frame-Options
  frameguard: { action: "deny" },

  // XSS Filter
  xssFilter: true,
});

// ã‚«ã‚¹ã‚¿ãƒ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢
const customSecurity = (req, res, next) => {
  // API ã‚­ãƒ¼ã®æ¤œè¨¼
  if (req.path.startsWith("/api/admin/")) {
    const apiKey = req.headers["x-api-key"];
    if (!apiKey || apiKey !== process.env.ADMIN_API_KEY) {
      return res.status(401).json({ error: "Unauthorized" });
    }
  }

  // ãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±ã®è¿½åŠ 
  res.setHeader("X-RateLimit-Limit", "100");
  res.setHeader("X-RateLimit-Remaining", "99");
  res.setHeader("X-RateLimit-Reset", Date.now() + 3600000);

  // ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æƒ…å ±ã®éš è”½
  res.removeHeader("X-Powered-By");
  res.removeHeader("Server");

  next();
};

module.exports = { securityHeaders, customSecurity };
```

### ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®š

**`deployment/setup-firewall.sh`**:

```bash
#!/bin/bash

# ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ

set -e

echo "ğŸ”¥ Setting up firewall..."

# UFW ã®æœ‰åŠ¹åŒ–
sudo ufw --force enable

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒªã‚·ãƒ¼
sudo ufw default deny incoming
sudo ufw default allow outgoing

# SSH ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆç®¡ç†ç”¨ï¼‰
sudo ufw allow ssh
sudo ufw allow 22/tcp

# HTTP/HTTPS
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒ¼ãƒˆï¼ˆå†…éƒ¨ã®ã¿ï¼‰
sudo ufw allow from 127.0.0.1 to any port 3000

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆå†…éƒ¨ã®ã¿ï¼‰
sudo ufw allow from 127.0.0.1 to any port 5432

# Redisï¼ˆå†…éƒ¨ã®ã¿ï¼‰
sudo ufw allow from 127.0.0.1 to any port 6379

# ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå†…éƒ¨ã®ã¿ï¼‰
sudo ufw allow from 127.0.0.1 to any port 9090  # Prometheus
sudo ufw allow from 127.0.0.1 to any port 3001  # Grafana

# ç‰¹å®šIPã‹ã‚‰ã®ç®¡ç†ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆå¿…è¦ã«å¿œã˜ã¦è¨­å®šï¼‰
# sudo ufw allow from YOUR_ADMIN_IP to any port 22

# ãƒ­ã‚°è¨­å®š
sudo ufw logging on

# è¨­å®šç¢ºèª
sudo ufw status verbose

echo "âœ… Firewall setup completed"
```

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### Node.js ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–

**`config/performance/pm2.config.js`**:

```javascript
module.exports = {
  apps: [
    {
      name: "portfolio-app",
      script: "server.js",
      instances: "max", // CPU ã‚³ã‚¢æ•°ã«å¿œã˜ã¦è‡ªå‹•èª¿æ•´
      exec_mode: "cluster",

      // ç’°å¢ƒè¨­å®š
      env: {
        NODE_ENV: "production",
        PORT: 3000,
      },

      // ãƒ¡ãƒ¢ãƒªç®¡ç†
      max_memory_restart: "1G",

      // ãƒ­ã‚°è¨­å®š
      log_file: "/var/www/portfolio/logs/pm2.log",
      out_file: "/var/www/portfolio/logs/pm2-out.log",
      error_file: "/var/www/portfolio/logs/pm2-error.log",
      log_date_format: "YYYY-MM-DD HH:mm:ss Z",

      // è‡ªå‹•å†èµ·å‹•è¨­å®š
      watch: false,
      ignore_watch: ["node_modules", "logs", "public/uploads"],

      // ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®š
      kill_timeout: 5000,
      wait_ready: true,
      listen_timeout: 10000,

      // ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
      health_check_grace_period: 3000,

      // ç’°å¢ƒå¤‰æ•°
      env_production: {
        NODE_ENV: "production",
        PORT: 3000,
        NODE_OPTIONS: "--max-old-space-size=1024",
      },
    },
  ],

  // ãƒ‡ãƒ—ãƒ­ã‚¤è¨­å®š
  deploy: {
    production: {
      user: "portfolio",
      host: "your-server.com",
      ref: "origin/main",
      repo: "git@github.com:your-repo/portfolio.git",
      path: "/var/www/portfolio",
      "post-deploy":
        "npm install && npm run build && pm2 reload ecosystem.config.js --env production",
    },
  },
};
```

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–

**`database/performance-tuning.sql`**:

```sql
-- PostgreSQL ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

-- 1. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æœ€é©åŒ–
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_portfolio_items_status_created
ON portfolio_items(status, created_at DESC)
WHERE status = 'published';

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_portfolio_items_categories_gin
ON portfolio_items USING GIN(categories);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_portfolio_items_tags_gin
ON portfolio_items USING GIN(tags);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_portfolio_items_effective_date_desc
ON portfolio_items(effective_date DESC NULLS LAST);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tag_info_name_lower
ON tag_info(LOWER(name));

-- 2. éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_portfolio_items_published
ON portfolio_items(created_at DESC)
WHERE status = 'published';

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_portfolio_items_draft
ON portfolio_items(updated_at DESC)
WHERE status = 'draft';

-- 3. è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_portfolio_items_status_categories_date
ON portfolio_items(status, categories, effective_date DESC);

-- 4. çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
ANALYZE portfolio_items;
ANALYZE tag_info;

-- 5. ãƒã‚­ãƒ¥ãƒ¼ãƒ è¨­å®š
-- è‡ªå‹•ãƒã‚­ãƒ¥ãƒ¼ãƒ è¨­å®šã®èª¿æ•´
ALTER TABLE portfolio_items SET (
  autovacuum_vacuum_scale_factor = 0.1,
  autovacuum_analyze_scale_factor = 0.05
);

-- 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®šã®ç¢ºèªç”¨ãƒ“ãƒ¥ãƒ¼
CREATE OR REPLACE VIEW performance_stats AS
SELECT
  schemaname,
  tablename,
  attname,
  n_distinct,
  correlation
FROM pg_stats
WHERE schemaname = 'public'
AND tablename IN ('portfolio_items', 'tag_info');

-- 7. ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªã®ç›£è¦–
-- postgresql.conf ã§è¨­å®š:
-- log_min_duration_statement = 1000  # 1ç§’ä»¥ä¸Šã®ã‚¯ã‚¨ãƒªã‚’ãƒ­ã‚°
-- log_statement = 'all'              # å…¨ã¦ã®ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆã‚’ãƒ­ã‚°
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®å®Ÿè£…

**`lib/cache/cache-strategy.js`**:

```javascript
const Redis = require("redis");
const LRU = require("lru-cache");

class CacheStrategy {
  constructor() {
    // Redisï¼ˆåˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
    this.redis = Redis.createClient({
      url: process.env.REDIS_URL,
      retry_strategy: (times) => Math.min(times * 50, 2000),
    });

    // LRUï¼ˆãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
    this.memoryCache = new LRU({
      max: 1000,
      ttl: 1000 * 60 * 5, // 5åˆ†
    });

    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥éšå±¤ã®å®šç¾©
    this.cacheHierarchy = {
      // L1: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆæœ€é€Ÿï¼‰
      memory: {
        ttl: 300, // 5åˆ†
        maxSize: 1000,
      },
      // L2: Redisï¼ˆé«˜é€Ÿï¼‰
      redis: {
        ttl: 1800, // 30åˆ†
        maxSize: 10000,
      },
      // L3: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆæœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼‰
      database: {
        ttl: 0, // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãªã„
      },
    };
  }

  async get(key, options = {}) {
    const { useMemory = true, useRedis = true } = options;

    try {
      // L1: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
      if (useMemory) {
        const memoryResult = this.memoryCache.get(key);
        if (memoryResult !== undefined) {
          this.recordCacheHit("memory");
          return memoryResult;
        }
      }

      // L2: Redisã‹ã‚‰å–å¾—
      if (useRedis) {
        const redisResult = await this.redis.get(key);
        if (redisResult) {
          const parsed = JSON.parse(redisResult);

          // ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚ä¿å­˜
          if (useMemory) {
            this.memoryCache.set(key, parsed);
          }

          this.recordCacheHit("redis");
          return parsed;
        }
      }

      this.recordCacheMiss();
      return null;
    } catch (error) {
      console.error("Cache get error:", error);
      return null;
    }
  }

  async set(key, value, options = {}) {
    const { ttl = 3600, useMemory = true, useRedis = true } = options;

    try {
      // ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
      if (useMemory) {
        this.memoryCache.set(key, value);
      }

      // Redisã«ä¿å­˜
      if (useRedis) {
        const serialized = JSON.stringify(value);
        if (ttl > 0) {
          await this.redis.setex(key, ttl, serialized);
        } else {
          await this.redis.set(key, serialized);
        }
      }

      return true;
    } catch (error) {
      console.error("Cache set error:", error);
      return false;
    }
  }

  async invalidate(pattern) {
    try {
      // ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç„¡åŠ¹åŒ–
      if (pattern.includes("*")) {
        // ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        const keys = Array.from(this.memoryCache.keys());
        const regex = new RegExp(pattern.replace(/\*/g, ".*"));
        keys.forEach((key) => {
          if (regex.test(key)) {
            this.memoryCache.delete(key);
          }
        });
      } else {
        this.memoryCache.delete(pattern);
      }

      // Redisã®ç„¡åŠ¹åŒ–
      const redisKeys = await this.redis.keys(pattern);
      if (redisKeys.length > 0) {
        await this.redis.del(...redisKeys);
      }

      return true;
    } catch (error) {
      console.error("Cache invalidation error:", error);
      return false;
    }
  }

  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®å®Ÿè£…
  async getWithFallback(key, fetchFunction, options = {}) {
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
    let result = await this.get(key, options);

    if (result === null) {
      // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
      try {
        result = await fetchFunction();

        if (result !== null && result !== undefined) {
          // å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
          await this.set(key, result, options);
        }
      } catch (error) {
        console.error("Fallback function error:", error);
        throw error;
      }
    }

    return result;
  }

  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã®è¨˜éŒ²
  recordCacheHit(type) {
    // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ï¼ˆPrometheusãªã©ï¼‰
    if (global.metrics) {
      global.metrics.cacheHits.inc({ type });
    }
  }

  recordCacheMiss() {
    if (global.metrics) {
      global.metrics.cacheMisses.inc();
    }
  }

  // ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
  async healthCheck() {
    try {
      // ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çŠ¶æ…‹
      const memoryStats = {
        size: this.memoryCache.size,
        max: this.memoryCache.max,
      };

      // Redisã®çŠ¶æ…‹
      await this.redis.ping();
      const redisInfo = await this.redis.info("memory");

      return {
        status: "healthy",
        memory: memoryStats,
        redis: { connected: true, info: redisInfo },
      };
    } catch (error) {
      return {
        status: "unhealthy",
        error: error.message,
      };
    }
  }
}

module.exports = new CacheStrategy();
```

### æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**`deployment/deploy-production.sh`**:

```bash
#!/bin/bash

# æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

set -e

# è¨­å®š
APP_DIR="/var/www/portfolio"
BACKUP_DIR="/var/backups/portfolio"
SERVICE_NAME="portfolio-app"
NGINX_SERVICE="nginx"

echo "ğŸš€ Starting production deployment..."

# 1. å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
echo "ğŸ” Checking prerequisites..."
if [ ! -f ".env.production" ]; then
    echo "âŒ .env.production file not found"
    exit 1
fi

if [ ! -d "$APP_DIR" ]; then
    echo "âŒ Application directory not found: $APP_DIR"
    exit 1
fi

# 2. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
echo "ğŸ’¾ Creating backup..."
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_PATH="$BACKUP_DIR/backup_$TIMESTAMP"

mkdir -p $BACKUP_PATH
cp -r $APP_DIR $BACKUP_PATH/
echo "âœ… Backup created: $BACKUP_PATH"

# 3. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åœæ­¢
echo "â¹ï¸ Stopping application..."
sudo systemctl stop $SERVICE_NAME || true
sudo pm2 stop all || true

# 4. ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ—ãƒ­ã‚¤
echo "ğŸ“¦ Deploying code..."
cd $APP_DIR

# Git pullï¼ˆæœ¬ç•ªç’°å¢ƒã®å ´åˆï¼‰
if [ -d ".git" ]; then
    git fetch origin
    git reset --hard origin/main
fi

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm ci --production

# ãƒ“ãƒ«ãƒ‰å®Ÿè¡Œ
npm run build

# 5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç§»è¡Œ
echo "ğŸ—„ï¸ Running database migrations..."
node database/migrate-production.js

# 6. ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
echo "ğŸ“ Setting up filesystem..."
./deployment/setup-filesystem.sh

# 7. SSLè¨­å®š
echo "ğŸ”’ Setting up SSL..."
./deployment/setup-ssl.sh

# 8. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
echo "âš™ï¸ Updating configuration..."
cp .env.production .env
sudo cp config/nginx/portfolio.conf /etc/nginx/sites-available/
sudo ln -sf /etc/nginx/sites-available/portfolio.conf /etc/nginx/sites-enabled/
sudo nginx -t

# 9. ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹
echo "â–¶ï¸ Starting services..."
sudo systemctl start $SERVICE_NAME
sudo systemctl reload $NGINX_SERVICE

# PM2ã§ã®èµ·å‹•ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç’°å¢ƒã®å ´åˆï¼‰
pm2 start config/performance/pm2.config.js --env production

# 10. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
echo "ğŸ¥ Performing health check..."
sleep 10

HEALTH_CHECK_URL="https://portfolio.example.com/api/health"
HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" $HEALTH_CHECK_URL)

if [ $HTTP_STATUS -eq 200 ]; then
    echo "âœ… Health check passed"
else
    echo "âŒ Health check failed (HTTP $HTTP_STATUS)"
    echo "ğŸ”„ Rolling back..."

    # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
    sudo systemctl stop $SERVICE_NAME
    pm2 stop all

    # æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ
    LATEST_BACKUP=$(ls -t $BACKUP_DIR | head -1)
    cp -r $BACKUP_DIR/$LATEST_BACKUP/* $APP_DIR/

    sudo systemctl start $SERVICE_NAME
    sudo systemctl reload $NGINX_SERVICE

    exit 1
fi

# 11. ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
echo "ğŸ“Š Setting up monitoring..."
sudo systemctl enable prometheus
sudo systemctl start prometheus
sudo systemctl enable grafana-server
sudo systemctl start grafana-server

# 12. æœ€çµ‚ç¢ºèª
echo "ğŸ” Final verification..."
curl -f $HEALTH_CHECK_URL > /dev/null
echo "âœ… Deployment completed successfully"

# 13. é€šçŸ¥é€ä¿¡
echo "ğŸ“§ Sending deployment notification..."
echo "Portfolio system deployed successfully at $(date)" | \
mail -s "Deployment Success" admin@example.com

echo "ğŸ‰ Production deployment completed!"
echo "ğŸ“‹ Summary:"
echo "  - Application URL: https://portfolio.example.com"
echo "  - Backup location: $BACKUP_PATH"
echo "  - Monitoring: https://portfolio.example.com:3001"
echo "  - Logs: $APP_DIR/logs/"
```

ã“ã®åŒ…æ‹¬çš„ãªæœ¬ç•ªç’°å¢ƒå¯¾å¿œã‚¬ã‚¤ãƒ‰ã«ã‚ˆã‚Šã€ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šã—ãŸæœ¬ç•ªé‹ç”¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
